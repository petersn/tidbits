{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving/loading numpy arrays efficiently.\n",
    "\n",
    "How should we save/load relatively large datasets efficiently to disk?\n",
    "Here we explore a bunch of options, and show some tradeoffs.\n",
    "Here we will be using the built-in MNIST dataset from Tensorflow, to realistically test compression.\n",
    "All tests here were run on an i3-6100 with a modern SSD.\n",
    "\n",
    "_Warning: Running this notebook will write a few hundred megabytes to disk._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import os, numpy\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Use `numpy.save`\n",
    "\n",
    "Uses the numpy \"`.npy`\" format, which is very simple: basically a trivial ~80 byte header encoding the array data type and shape followed by the raw numpy array contents.\n",
    "[Documented here.](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.save.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Megabytes on disk: 164.489822388\n",
      "CPU times: user 0 ns, sys: 112 ms, total: 112 ms\n",
      "Wall time: 114 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "numpy.save(\"/tmp/dump1.npy\", mnist.train.images)\n",
    "print \"Megabytes on disk:\", os.path.getsize(\"/tmp/dump1.npy\") * 2**-20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Use `numpy.savez`\n",
    "\n",
    "Creates a _non_-compressed zip archive that contains the array as a `.npy`. Lets you easily save multiple arrays into one archive, and give them names, which is pretty nifty. [Documented here.](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.savez.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Megabytes on disk: 164.489936829\n",
      "CPU times: user 156 ms, sys: 244 ms, total: 400 ms\n",
      "Wall time: 447 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "numpy.savez(\"/tmp/dump2.npz\", dataset=mnist.train.images)\n",
    "print \"Megabytes on disk:\", os.path.getsize(\"/tmp/dump2.npz\") * 2**-20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Use `numpy.savez_compressed`\n",
    "\n",
    "Basically the same as `numpy.savez`, except it enables zip DEFLATE compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Megabytes on disk: 13.8666152954\n",
      "CPU times: user 2.37 s, sys: 180 ms, total: 2.55 s\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "numpy.savez_compressed(\"/tmp/dump3.npz\", dataset=mnist.train.images)\n",
    "print \"Megabytes on disk:\", os.path.getsize(\"/tmp/dump3.npz\") * 2**-20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 4: Take things into our own hands and use zlib compression.\n",
    "\n",
    "Another option is to take things into our own hands and compress the array ourself using zlib.\n",
    "This doesn't store any information about the datatype or array shape, so we have to remember those separately ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Megabytes on disk: 13.7638893127\n",
      "CPU times: user 11.2 s, sys: 32 ms, total: 11.2 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import zlib\n",
    "compressor = zlib.compressobj(9)\n",
    "with open(\"/tmp/dump4.z\", \"wb\") as f:\n",
    "    f.write(compressor.compress(buffer(mnist.train.images)))\n",
    "    f.write(compressor.flush())\n",
    "print \"Megabytes on disk:\", os.path.getsize(\"/tmp/dump4.z\") * 2**-20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 5: Take things into our own hands and use bz2 compression.\n",
    "\n",
    "Same as above, but with another compression library that is also built into Python (and has a nicer interface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Megabytes on disk: 7.82636451721\n",
      "CPU times: user 3.62 s, sys: 16 ms, total: 3.64 s\n",
      "Wall time: 3.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import bz2\n",
    "with bz2.BZ2File(\"/tmp/dump5.bz2\", \"wb\") as f:\n",
    "    f.write(buffer(mnist.train.images))\n",
    "print \"Megabytes on disk:\", os.path.getsize(\"/tmp/dump5.bz2\") * 2**-20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data.\n",
    "\n",
    "Here are corresponding methods for loading our arrays back again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 60 ms, total: 64 ms\n",
      "Wall time: 61.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "load1 = numpy.load(\"/tmp/dump1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 140 ms, sys: 52 ms, total: 192 ms\n",
      "Wall time: 193 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "archive2 = numpy.load(\"/tmp/dump2.npz\")\n",
    "load2 = archive2[\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 392 ms, sys: 56 ms, total: 448 ms\n",
      "Wall time: 454 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "archive3 = numpy.load(\"/tmp/dump3.npz\")\n",
    "load3 = archive3[\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 304 ms, sys: 64 ms, total: 368 ms\n",
      "Wall time: 375 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"/tmp/dump4.z\", \"rb\") as f:\n",
    "    data = f.read().decode(\"zlib\")\n",
    "# Note how we have to separately remember the array data type and shape. :(\n",
    "load4 = numpy.fromstring(data, dtype=numpy.float32).reshape((55000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.93 s, sys: 96 ms, total: 3.03 s\n",
      "Wall time: 3.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"/tmp/dump5.bz2\", \"rb\") as f:\n",
    "    data = f.read().decode(\"bz2\")\n",
    "load5 = numpy.fromstring(data, dtype=numpy.float32).reshape((55000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All loaded arrays loaded correctly.\n"
     ]
    }
   ],
   "source": [
    "# Final sanity check that all loads were correct.\n",
    "for test_array in (load1, load2, load3, load4, load5):\n",
    "    assert numpy.array_equal(mnist.train.images, test_array)\n",
    "print \"All loaded arrays loaded correctly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion.\n",
    "\n",
    "We saw that `numpy.load` was the fastest option for both saving and loading, but offers no compression and doesn't let us save multiple arrays with distinct names.\n",
    "\n",
    "The compression offered by `numpy.savez_compressed` reduced the file to about 8.4% of the size at the cost of taking about a dozen times as long to save and a few times longer to load.\n",
    "\n",
    "Finally, by taking things into our own hands and using bz2 to compress we managed to get the file compressed down to about 4.8% of its original size, with a save time only a little longer than `numpy.savez_compressed`, but a load time that was over 8 times longer, and over 60 times longer than loading the uncompressed result of `numpy.save`.\n",
    "However, we unfortunately have to keep track of the array shape and datatype ourself, which sucks a lot.\n",
    "\n",
    "Option 4 (using zlib manually ourselves) was lackluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
