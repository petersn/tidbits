{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained MDPs\n",
    "\n",
    "This is my (Peter Schmidt-Nielsen's) attempt to follow along and implement what was described in an email written by Tong Mu.\n",
    "\n",
    "There are three parts:\n",
    "1. Finding the optimal policy for a known MDP. (via value iteration)\n",
    "2. Finding the optimal policy for a known MDP. (via linear programming)\n",
    "3. Finding the optimal steady-state policy for a known MDP subject to a constraint. (via linear programming)\n",
    "\n",
    "Finally, there's an observation that (2) and (3) are duals of each other!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Unconstrained MDP, via value iteration\n",
    "\n",
    "To start with, let's do plain old value iteration for an MDP. We'll start by generating a random MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def make_mdp(state_count, action_count):\n",
    "    # Make some random transition probabilities.\n",
    "    transition_probs = np.exp(5 * np.random.randn(state_count, action_count, state_count))\n",
    "    transition_probs /= transition_probs.sum(axis=-1)[..., np.newaxis]\n",
    "\n",
    "    # Make some random transitions give some random reward, biasing towards penalties.\n",
    "    rewards = np.random.randint(-17, 10, size=(state_count, action_count, state_count))\n",
    "\n",
    "    return {\n",
    "        \"state_count\": state_count,\n",
    "        \"action_count\": action_count,\n",
    "        # trans[s, a, s'] is the probability of transitioning from s to s' under action a.\n",
    "        \"trans\": transition_probs,\n",
    "        # rewards[s, a, s'] is the reward given by transitioning from s to s' from action a.\n",
    "        \"rewards\": rewards,\n",
    "    }\n",
    "\n",
    "main_mdp = make_mdp(\n",
    "    state_count=10,\n",
    "    action_count=2,\n",
    ")\n",
    "gamma = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do value iteration, we iterate the following to find a fixed point $V$ (solution to the Bellman equation):\n",
    "$$ V'(s) = \\max_a \\sum_{s'} \\mathrm{trans}[s, a, s'] \\, \\big(\\mathrm{rewards}[s, a, s'] + \\gamma V(s')\\big) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.58873285,  12.12336257,  -9.41499433,  -4.56413414,\n",
       "       -13.10467415,   1.55688255,   9.24490558,   4.24928972,\n",
       "         1.15689845,   1.02660245])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def value_iteration(mdp, gamma, iterations=100):\n",
    "    values = np.zeros(shape=mdp[\"state_count\"])\n",
    "    for _ in range(iterations):\n",
    "        values = (mdp[\"trans\"] * (mdp[\"rewards\"] + gamma * values)).sum(axis=-1).max(axis=-1)\n",
    "    return values\n",
    "\n",
    "value_iteration(main_mdp, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Unconstrained MDP, via linear programming\n",
    "\n",
    "(Per Mu, personal communication, 14th Dec 2020)\n",
    "\n",
    "Observe that claiming $x = \\max_i y_i$ is equivalent to claiming that $x$ is the minimal value such that $\\forall i, x \\ge y_i$.\n",
    "We will use this observation to rewrite each of the maximizations in the Bellman equation from before into a collection of linear constraints.\n",
    "In particular, we want:\n",
    "$$ \\forall s, V(s) = \\max_a \\sum_{s'} \\mathrm{trans}[s, a, s'] \\, \\big(\\mathrm{rewards}[s, a, s'] + \\gamma V(s')\\big) $$\n",
    "Thus, for each $s$ we get a collection of constraints for each action $a$ implementing a single max operator:\n",
    "$$\\begin{align}\n",
    "    \\forall s \\forall a, \\quad V(s) &\\ge \\sum_{s'} \\mathrm{trans}[s, a, s'] \\, \\big(\\mathrm{rewards}[s, a, s'] + \\gamma V(s')\\big) \\\\\n",
    "    \\underbrace{V(s) - \\gamma \\sum_{s'} \\mathrm{trans}[s, a, s'] \\, V(s')}_{\\text{Entries in the constraint's row}} &\\ge \\underbrace{\\sum_{s'} \\mathrm{trans}[s, a, s'] \\, \\mathrm{rewards}[s, a, s']}_{\\text{Constant term on the constraint}}\n",
    "\\end{align}$$\n",
    "This is precisely a linear program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "def linprog_values(mdp, gamma):\n",
    "    # Our constraint matrix must have shape [constraint_count, variable_count],\n",
    "    # where constraint_count = state_count * action_count, and variable_count = state_count.\n",
    "\n",
    "    # Our constraint matrix has two terms, corresponding to the two terms in the LHS above.\n",
    "    # First, the V(s) term:\n",
    "    constraint_matrix = np.repeat(np.eye(mdp[\"state_count\"]), mdp[\"action_count\"], axis=0)\n",
    "    # Second, the -γ sum_{s'} trans[s, a, s'] V(s') term:\n",
    "    constraint_matrix += -gamma * mdp[\"trans\"].reshape((-1, mdp[\"state_count\"]))\n",
    "\n",
    "    # Our constraint bounds correspond to the RHS above:\n",
    "    constraint_bounds = (mdp[\"trans\"] * mdp[\"rewards\"]).sum(axis=-1).flatten()\n",
    "\n",
    "    result = scipy.optimize.linprog(\n",
    "        # Because our goal is simply to minimize V, any positive objective whatsoever suffices.\n",
    "        # (cf. Mu, personal communication.)\n",
    "        c=np.ones(mdp[\"state_count\"]),\n",
    "        # Note that we'd like a $\\ge$ constraint, so we negate both the constraint and bounds.\n",
    "        A_ub=-constraint_matrix,\n",
    "        b_ub=-constraint_bounds,\n",
    "        bounds=(None, None),\n",
    "    )\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Linear program failed: \" + result.message)\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With any luck this LP based solution will equal the value iteration solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  3.58873285,  12.12336257,  -9.41499433,  -4.56413414,\n",
       "       -13.10467415,   1.55688255,   9.24490558,   4.24928972,\n",
       "         1.15689845,   1.02660245])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linprog_values(main_mdp, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.58873285,  12.12336257,  -9.41499433,  -4.56413414,\n",
       "       -13.10467415,   1.55688255,   9.24490558,   4.24928972,\n",
       "         1.15689845,   1.02660245])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration(main_mdp, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huzzah, the two results are equal, it worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Constrained MDP, via linear programming\n",
    "\n",
    "(Also per Mu, personal communication, 14th Dec 2020)\n",
    "\n",
    "Now imagine each $(\\text{s}, \\text{a})$ pair has an associated constraint-cost $\\mathrm{constraint\\_costs}[s, a]$ and we only want a policy that, in steady-state, consumes no more than $\\delta$ units of constraint-cost in expectation per step.\n",
    "\n",
    "To do this, we compute an occupancy measure $y(s, a)$, giving a distribution over which edges (i.e. state-action pairs) we take under the optimal policy in steady-state.\n",
    "We know that the occupancy measure must be a normalized probability distribution:\n",
    "$$ \\sum_{s, a} y(s, a) = 1 $$\n",
    "Additionally, we know that all of the probability flowing into a state in the MDP must equal all of the probability flowing out of it:\n",
    "$$ \\forall s', \\quad \\underbrace{\\sum_{s, a} \\mathrm{trans}[s, a, s'] \\, y(s, a)}_{\\text{Probability flowing into $s'$}} = \\underbrace{\\sum_{a} y(s', a)}_{\\text{Probability flowing out of $s'$}} $$\n",
    "In this framing we may now easily encode our constraint-cost limit of $\\delta$:\n",
    "$$ \\sum_{s, a} \\mathrm{constraint\\_costs}[s, a] \\, y(s, a) \\le \\delta $$\n",
    "Finally, we would like to maximize the expected reward of an action we take in steady-state, namely the quantity:\n",
    "$$ \\sum_{s, a, s'} \\mathrm{trans}[s, a, s'] \\, \\mathrm{rewards}[s, a, s'] \\, y(s, a) $$\n",
    "The above form a linear program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constraint_costs[s, a] gives the constraint-cost of taking action a from state s.\n",
    "constraint_costs = np.random.uniform(0, 1, size=(main_mdp[\"state_count\"], main_mdp[\"action_count\"]))\n",
    "\n",
    "def constrained_linprog_occupancy(mdp, constraint_costs, delta):\n",
    "    # Our equality constraint matrix must have shape [constraint_count, variable_count],\n",
    "    # where constraint_count = state_count + 1, and variable_count = state_count * action_count.\n",
    "    \n",
    "    # Our constraint matrix has two terms: the probability flowing in, and out.\n",
    "    # First, the probability flowing into s':\n",
    "    equality_constraint_matrix = mdp[\"trans\"].reshape((-1, mdp[\"state_count\"])).T.copy()\n",
    "    # Second, the probability flowing out of s':\n",
    "    equality_constraint_matrix -= np.repeat(np.eye(mdp[\"state_count\"]), mdp[\"action_count\"], axis=0).T\n",
    "\n",
    "    # Add a final constraint enforcing normalization of the occupancy measure.\n",
    "    equality_constraint_matrix = np.append(\n",
    "        equality_constraint_matrix, [[1] * mdp[\"state_count\"] * mdp[\"action_count\"]], axis=0,\n",
    "    )\n",
    "\n",
    "    objective_vector = (mdp[\"trans\"] * mdp[\"rewards\"]).sum(axis=-1).flatten()\n",
    "\n",
    "    result = scipy.optimize.linprog(\n",
    "        # Here we negate because we'd like to maximize instead of minimize.\n",
    "        c=-objective_vector,\n",
    "        A_eq=equality_constraint_matrix,\n",
    "        b_eq=[0.0] * mdp[\"state_count\"] + [1.0],\n",
    "        # Demand that we don't exceed our constraint-cost budget.\n",
    "        A_ub=[constraint_costs.flatten()],\n",
    "        b_ub=[delta],\n",
    "    )\n",
    "    if not result.success:\n",
    "        raise ValueError(\"Linear program failed: \" + result.message)\n",
    "    occupancy_under_optimal_policy = result.x\n",
    "    return {\n",
    "        \"score\": objective_vector @ occupancy_under_optimal_policy,\n",
    "        \"occupancy\": occupancy_under_optimal_policy.reshape((mdp[\"state_count\"], mdp[\"action_count\"])),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-06fafc557bba>:21: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  result = scipy.optimize.linprog(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.22227704642581816,\n",
       " 'occupancy': array([[2.12490592e-01, 3.57631227e-12],\n",
       "        [1.88910269e-01, 5.46803836e-13],\n",
       "        [9.22043930e-04, 5.94134483e-12],\n",
       "        [6.14471187e-12, 1.62865323e-01],\n",
       "        [1.07324593e-12, 3.22565915e-02],\n",
       "        [5.79259415e-02, 1.05241766e-01],\n",
       "        [1.66459874e-12, 7.21913905e-04],\n",
       "        [2.18737999e-12, 1.89920403e-01],\n",
       "        [3.83926718e-12, 2.77940301e-02],\n",
       "        [2.09511265e-02, 4.62493900e-12]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constrained_linprog_occupancy(\n",
    "    mdp=main_mdp,\n",
    "    constraint_costs=constraint_costs,\n",
    "    delta=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this out, let's see how the best score we can achieve varies as a function of our total constraint-cost budget, $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-06fafc557bba>:21: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  result = scipy.optimize.linprog(\n",
      "/usr/local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "xs = np.linspace(0, 1)\n",
    "ys = []\n",
    "for delta in xs:\n",
    "    try:\n",
    "        score = constrained_linprog_occupancy(main_mdp, constraint_costs, delta)[\"score\"]\n",
    "        ys.append(score)\n",
    "    except ValueError:\n",
    "        ys.append(None)\n",
    "plt.plot(xs, ys)\n",
    "plt.ylabel(\"Expected per-step steady-state reward\\nof the optimal constrained policy\")\n",
    "_ = plt.xlabel(\"Total constraint-cost budget, δ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, the best score we can achieve goes up as we give more constraint budget.\n",
    "\n",
    "As a final sanity check, if the constraint budget is high enough then we should simply get the unconstrained solution.\n",
    "This should be extremely close to the score we get from our program of section (2) in the limit of $\\gamma$ very close to 1, but rescaling the score back down.\n",
    "For example, the program of section (2) with $\\gamma = 0.999$ should give approximately 1,000 times the expected per-step reward of the steady-state solution as the value of every state in the MDP. (It should give approximately the same value for every state because with a $\\gamma = 0.999$ it's really cheap to just walk from the worst state to the best state!)\n",
    "\n",
    "To check this, we compare the output of the program from section (2) with the output of the program of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.56277493, 0.563687  , 0.56095741, 0.56179701, 0.56077652,\n",
       "       0.56228675, 0.56305172, 0.56285213, 0.56277044, 0.56220483])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approximately compute the expected steady-state per-step reward using our previous program.\n",
    "linprog_values(main_mdp, 1 - 10**-4) / 10**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-06fafc557bba>:21: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  result = scipy.optimize.linprog(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5629318775359461"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare to the per-step reward we get with our new program, making delta so high as to be unconstrained.\n",
    "constrained_linprog_occupancy(main_mdp, constraint_costs, delta=1000)[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huzzah! The values nearly agree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duality\n",
    "\n",
    "Okay, wait, what!?\n",
    "\n",
    "I was working on this, and when writing the program of section (3) I noticed that the LP I was writing seemed to be (basically) the asymmetric dual of the LP constructed in the program of section (2)!\n",
    "In particular, the duality between:\n",
    "\n",
    "1. Maximize $c^T x$ subject to $Ax \\le b$. (As constructed in `linprog_values`.)\n",
    "2. Minimize $b^T y$ subject to $A^T y = c, y \\ge 0$. (As constructed in `constrained_linprog_occupancy`.)\n",
    "\n",
    "In particular, note that (before the concatenation of the normalization constraint) `equality_constraint_matrix` as constructed in section 3 is _exactly_ the transpose of `constraint_matrix` from section 2!\n",
    "Also, the `objective_vector` in section 3 is _exactly_ the `constraint_bounds` from section 2!\n",
    "\n",
    "What is going on here? This seems so cool!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
